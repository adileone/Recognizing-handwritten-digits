{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![picture1.png](picture1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
>>>>>>> bff684a5855adf4466e2abcf0da80f5feeba7a3b
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'nb_epoch': 3}\n"
     ]
    }
   ],
   "source": [
    "# imports for array-handling and plotting\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# keras imports for the dataset and building our neural network\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "# sklearn imports for the model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time\n",
    "\n",
    "#Data-preparation\n",
    "\n",
    "images = np.loadtxt(\"handwritten_digits_images.csv\", delimiter=',')\n",
    "labels = np.loadtxt(\"handwritten_digits_labels.csv\", delimiter=',')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels,test_size=0.3, random_state=32)\n",
    "\n",
    "#reshape data to fit model\n",
    "X_train = X_train.reshape(49000,28,28,1)\n",
    "X_test = X_test.reshape(21000,28,28,1)\n",
    "\n",
    "#Normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(10,kernel_size=5, activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(20, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',    metrics=['accuracy', f1, precision, recall])   \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def tuning(X_train,Y_train,X_test,Y_test):\n",
    "\n",
    "    \n",
    "    epochs = list(range(1,5))\n",
    "    param_grid = dict(nb_epoch=epochs)\n",
    "\n",
    "    k_model = KerasClassifier(build_fn=make_model, verbose=0)\n",
    "   \n",
    "    clf = GridSearchCV(estimator=k_model, param_grid=param_grid, \n",
    "                                   cv=5,\n",
    "                                   scoring=\"accuracy\", verbose=0 ,n_jobs=-1)\n",
    "    clf.fit(X_train,Y_train)\n",
    "    \n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    \n",
    "    model=make_model()\n",
    "    \n",
    "    return clf,model\n",
    "\n",
    "tune_start = time.time()\n",
    "clf = tuning(X_train,y_train,X_test,y_test)\n",
    "tune_end = time.time()\n",
    "\n",
    "tuning_time=tune_end-tune_start"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 4,
>>>>>>> bff684a5855adf4466e2abcf0da80f5feeba7a3b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49000 samples, validate on 21000 samples\n",
<<<<<<< HEAD
      "Epoch 1/4\n",
      "49000/49000 [==============================] - 14s 284us/step - loss: 0.0650 - acc: 0.9801 - f1: 0.9802 - precision: 0.9827 - recall: 0.9779 - val_loss: 0.0451 - val_acc: 0.9857 - val_f1: 0.9859 - val_precision: 0.9876 - val_recall: 0.9843\n",
      "Epoch 2/4\n",
      "49000/49000 [==============================] - 15s 308us/step - loss: 0.0543 - acc: 0.9836 - f1: 0.9836 - precision: 0.9856 - recall: 0.9817 - val_loss: 0.0441 - val_acc: 0.9861 - val_f1: 0.9862 - val_precision: 0.9872 - val_recall: 0.9851\n",
      "Epoch 3/4\n",
      "49000/49000 [==============================] - 15s 313us/step - loss: 0.0465 - acc: 0.9850 - f1: 0.9853 - precision: 0.9867 - recall: 0.9838 - val_loss: 0.0398 - val_acc: 0.9871 - val_f1: 0.9875 - val_precision: 0.9886 - val_recall: 0.9864\n",
      "Epoch 4/4\n",
      "49000/49000 [==============================] - 15s 314us/step - loss: 0.0424 - acc: 0.9863 - f1: 0.9865 - precision: 0.9878 - recall: 0.9852 - val_loss: 0.0368 - val_acc: 0.9888 - val_f1: 0.9890 - val_precision: 0.9897 - val_recall: 0.9883\n"
=======
      "Epoch 1/3\n",
      "49000/49000 [==============================] - 13s 269us/step - loss: 0.2717 - acc: 0.9170 - f1: 0.9034 - precision: 0.9300 - recall: 0.8869 - val_loss: 0.0949 - val_acc: 0.9718 - val_f1: 0.9720 - val_precision: 0.9769 - val_recall: 0.9673\n",
      "Epoch 2/3\n",
      "49000/49000 [==============================] - 14s 282us/step - loss: 0.0978 - acc: 0.9697 - f1: 0.9702 - precision: 0.9749 - recall: 0.9657 - val_loss: 0.0619 - val_acc: 0.9825 - val_f1: 0.9826 - val_precision: 0.9855 - val_recall: 0.9800\n",
      "Epoch 3/3\n",
      "49000/49000 [==============================] - 15s 310us/step - loss: 0.0719 - acc: 0.9780 - f1: 0.9783 - precision: 0.9814 - recall: 0.9753 - val_loss: 0.0521 - val_acc: 0.9845 - val_f1: 0.9843 - val_precision: 0.9866 - val_recall: 0.9821\n"
>>>>>>> bff684a5855adf4466e2abcf0da80f5feeba7a3b
     ]
    }
   ],
   "source": [
    "model=clf[1]\n",
    "\n",
    "y_train_enc = to_categorical(y_train)\n",
    "y_test_enc = to_categorical(y_test)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train_enc, validation_data=(X_test, y_test_enc), epochs=3)\n",
    "end = time.time()\n",
    "\n",
    "exec_time = end - start\n",
    "\n",
    "evaluation = model.evaluate(X_test, y_test_enc, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 5,
>>>>>>> bff684a5855adf4466e2abcf0da80f5feeba7a3b
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = evaluation[0]\n",
    "accuracy = evaluation[1]\n",
    "f1_measure = evaluation[2]\n",
    "precision = evaluation[3]\n",
    "recall = evaluation[4]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 11,
>>>>>>> bff684a5855adf4466e2abcf0da80f5feeba7a3b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      "Test Loss :  0.03681450295315257\n",
      "Test Accuracy :  98.88 %\n",
      "F1-Measure 0.989\n",
      "Precision 0.9897\n",
      "Recall 0.9883\n",
      "Cross Validation Time :  222.95 sec\n",
      "Training Time :  59.78 sec\n",
      "\n",
      "Confusion Matrix : \n",
      "\n",
      "[[2030    0    1    1    0    0    3    0    1    2]\n",
      " [   0 2370    9    0    2    0    1    3    3    0]\n",
      " [   1    0 2048    0    0    0    0    9    2    1]\n",
      " [   1    0   14 2122    0   11    0    6    4    4]\n",
      " [   3    1    1    0 2020    0    5    8    1   17]\n",
      " [   1    0    0    6    0 1861    6    0    5    1]\n",
      " [   4    2    1    0    2    1 2060    0    4    0]\n",
      " [   0    3    8    0    3    0    0 2145    3    3]\n",
      " [   1    1    6    2    1    8    2    3 2016   10]\n",
      " [   4    0    2    0    6    9    0   11    1 2093]]\n",
      "\n",
      "\n",
      "20765  classified correctly\n",
      "235  classified incorrectly\n",
      "\n",
      "Error rate :  1.12 %\n"
=======
      "Test Loss :  0.052068482326787145\n",
      "Test Accuracy :  98.45 %\n",
      "F1-Measure 98.4341\n",
      "Precision 98.6622\n",
      "Recall 98.2143\n",
      "Cross Validation Time :  225.95 sec\n",
      "Training Time :  42.43 sec\n",
      "\n",
      "Confusion Matrix : \n",
      "\n",
      "[[2012    0    6    1    1    2    8    0    2    6]\n",
      " [   1 2359   14    0    2    1    3    4    3    1]\n",
      " [   0    1 2042    6    0    0    0    8    3    1]\n",
      " [   1    0   18 2129    0    3    0    3    3    5]\n",
      " [   2    2    3    0 2027    0    5    5    1   11]\n",
      " [   1    1    5   14    1 1839   12    2    4    1]\n",
      " [   4    1    0    0    2    2 2063    0    2    0]\n",
      " [   0    5   22    2    3    0    0 2128    2    3]\n",
      " [   4    9    7    6    6    9    3    5 1989   12]\n",
      " [   3    4    0    6    4    8    1   10    4 2086]]\n",
      "\n",
      "\n",
      "20674  classified correctly\n",
      "326  classified incorrectly\n",
      "\n",
      "Error rate :  1.55 %\n"
>>>>>>> bff684a5855adf4466e2abcf0da80f5feeba7a3b
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Test Loss : \", loss)\n",
    "print(\"Test Accuracy : \", round(accuracy*100, 2), \"%\")\n",
<<<<<<< HEAD
    "print(\"F1-Measure\", round(f1_measure, 4))\n",
    "print(\"Precision\", round(precision, 4))\n",
    "print(\"Recall\", round(recall, 4))\n",
=======
    "print(\"F1-Measure\", round(f1_measure*100, 4))\n",
    "print(\"Precision\", round(precision*100, 4))\n",
    "print(\"Recall\", round(recall*100, 4))\n",
>>>>>>> bff684a5855adf4466e2abcf0da80f5feeba7a3b
    "print(\"Cross Validation Time : \", round(tuning_time, 2), \"sec\" )\n",
    "print(\"Training Time : \",  round(exec_time, 2), \"sec\"  )\n",
    "\n",
    "\n",
    "predicted_classes = model.predict_classes(X_test)\n",
<<<<<<< HEAD
    "conmat = confusion_matrix(y_test, predicted_classes)\n",
=======
    "cm = confusion_matrix(y_test, predicted_classes)\n",
>>>>>>> bff684a5855adf4466e2abcf0da80f5feeba7a3b
    "\n",
    "print()\n",
    "print('Confusion Matrix : ')\n",
    "print()\n",
<<<<<<< HEAD
    "print(conmat)\n",
=======
    "print(cm)\n",
>>>>>>> bff684a5855adf4466e2abcf0da80f5feeba7a3b
    "print()\n",
    "\n",
    "# see which we predicted correctly and which not ----- Indices of elements that are non-zero.\n",
    "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]\n",
    "\n",
    "print()\n",
    "print(len(correct_indices),\" classified correctly\")\n",
    "print(len(incorrect_indices),\" classified incorrectly\")\n",
    "\n",
    "error_rate = len(incorrect_indices)/(len(correct_indices)+len(incorrect_indices))\n",
    "\n",
    "print()\n",
    "print(\"Error rate : \", round(error_rate*100, 2), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
